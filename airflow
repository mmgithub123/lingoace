
operator->task->dag


DAG: The work (tasks), and the order in which work should take place (dependencies), written in Python.
DAG Run: An instance of a DAG for a particular logical date and time.
Operator: A class that acts as a template for carrying out some work.
Task: Defines work by implementing an operator, written in Python.
Task Instance: An instance of a task - that has been assigned to a DAG and has a state associated with a specific DAG run (i.e for a specific execution_date).
execution_date: The logical date and time for a DAG Run and its Task Instances.


DAG Runs
A DAG run is a physical instance of a DAG, containing task instances that run for a specific execution_date.




tasks:
Each task is an implementation of an Operator, for example a PythonOperator to execute some Python code,
or a BashOperator to run a Bash command.

 task_1 >> task_2 # Define dependencies
 We can say that task_1 is upstream of task_2, and conversely task_2 is downstream of task_1. 
 When a DAG Run is created, task_1 will start running and task_2 waits for task_1 to complete successfully before it may start.
 
 
 task state
 Task instances also have an indicative state, which could be “running”, “success”, “failed”, “skipped”, “up for retry”, etc.
 
 Airflow does have a feature for operator cross-communication called XCom that is described in the section XComs
 
 
 
 
 Airflow provides operators for many common tasks, including:

BashOperator - executes a bash command

PythonOperator - calls an arbitrary Python function

EmailOperator - sends an email

SimpleHttpOperator - sends an HTTP request

MySqlOperator, SqliteOperator, PostgresOperator, MsSqlOperator, OracleOperator, JdbcOperator, etc. - executes a SQL command

Sensor - an Operator that waits (polls) for a certain time, file, database row, S3 key, etc…

In addition to these basic building blocks, there are many more specific operators: DockerOperator, HiveOperator, S3FileTransformOperator, PrestoToMySqlTransfer, SlackAPIOperator… you get the idea!

Operators are only loaded by Airflow if they are assigned to a DAG.



-----------------------------------------------------------------------------------------
action:

查看依赖：
# print the list of active DAGs
airflow list_dags

# prints the list of tasks the "tutorial" dag_id
airflow list_tasks tutorial

# prints the hierarchy of tasks in the tutorial DAG
airflow list_tasks tutorial --tree


测试：
execution_date
# command layout: command subcommand dag_id task_id date
airflow test tutorial print_date 2015-06-01

note: airflow test,It simply allows testing a single task instance.


depends_on_past = true
是否满足 task 的依赖性。 直接位于 task 上游的任务实例需要处于success状态。 此外，如果设置depends_on_past=True ，除了满足上游成功之外，
前一个调度周期的 task instance 也需要成功（除非该 task 是第一次运行）
You may also want to consider wait_for_downstream=True when using depends_on_past=True. While depends_on_past=True causes a task instance to depend on 
the success of its previous task_instance, wait_for_downstream=True will cause a task instance to also wait for all task instances immediately downstream 
of the previous task instance to succeed.

运行：
backfill：
The operation of running a DAG for a specified date in the past is called “backfilling.”
》》》airflow backfill -s 2019-01-01 -e 2019-01-04 test_dag
In this case, I am running the test_dag DAG with the execution date set to 2019-01-01, 2019-01-02, and 2019-01-03.
